{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP37AOb2aGKqPZzFv23c3MF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Deep Learning Exercise 6 - Time Series Classification\n","\n","This exercise is about comparison of models suitable to time series classification for univariate and multivariete data.\n","\n","Data we will use come from [Time Series Classification Website](https://www.timeseriesclassification.com/dataset.php), we will use sensor data from FordA and Siemens datasets.\n","\n","Other datasets are also available, we will show you how to create your own as well.\n","\n","[Open in Google colab](https://colab.research.google.com/github/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_06_time_series_classification.ipynb) [Download from Github](https://raw.githubusercontent.com/jplatos/VSB-FEI-Deep-Learning/main/dl_06_time_series_classification.ipynb)\n"],"metadata":{"id":"azCNyrlr4NRT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wswCpGS5Zykp"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","tf.version.VERSION"]},{"cell_type":"markdown","source":["Lets download [FordA](https://www.timeseriesclassification.com/description.php?Dataset=FordA) dataset converted for our purposes to the [Feather file format](https://arrow.apache.org/docs/python/feather.html), a binary file format for data exchange.\n","\n","The data originates from ARFF file format used in Weka Data analysis tool and has classes labeled $\\{-1,1\\}$ which is not suitable for TensorFlow (SKLearn has no trouble with it), so we have to confert it to the $\\{0,1\\}$ set. "],"metadata":{"id":"tSLaQi8I6QTe"}},{"cell_type":"code","source":["train = pd.read_feather('https://github.com/jplatos/VSB-FEI-Deep-Learning/blob/941b1912c0971bef3c2ace907de4883bac8a88a6/datasets/FordA_TRAIN.feather?raw=true')\n","test = pd.read_feather('https://github.com/jplatos/VSB-FEI-Deep-Learning/blob/941b1912c0971bef3c2ace907de4883bac8a88a6/datasets/FordA_TRAIN.feather?raw=true')\n","train.target.replace({-1:0}, inplace=True)\n","test.target.replace({-1:0}, inplace=True)\n","print('Train: ',train.shape)\n","print('Test: ', test.shape)"],"metadata":{"id":"QfNkd6fCduIJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data contain 500 time steps of a measurement and single target value. The time series is almost normalized and it is not necessary to deal with it using scaling or normalizing. It may slightly improve the results but it depends on your experiments. "],"metadata":{"id":"YxoiE92S7Sa8"}},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"UMbG_GDOeakc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Parallel Coordinate plot is slightly difficult in MatPlotLib but this demonstration suffices. Other libraries may works better. But as you see it is very difficult to see many differences between the time series."],"metadata":{"id":"k-4AebhZ9izQ"}},{"cell_type":"code","source":["colors = ['b', 'g']\n","plt.figure(figsize=(21,9))\n","for idx in range(100):\n","  plt.plot(train.iloc[idx][:-1], c=colors[int(train.iloc[idx][-1])])\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"WbWLgyhuey7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert the data into numpy arrays and separates *X* and *y* data from each other for triaingn and testing data."],"metadata":{"id":"Gbo9ncXP92UZ"}},{"cell_type":"code","source":["train_x, train_y = train.drop(columns=['target']).values, train.target.values\n","test_x, test_y = test.drop(columns=['target']).values, test.target.values"],"metadata":{"id":"n6WzwPhUe_w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Simple accuracy metric computed as well as confusion matrix display."],"metadata":{"id":"-jVGUz_4-EYx"}},{"cell_type":"code","source":["def compute_metrics(y_true, y_pred, show_confusion_matrix=False):\n","  print(f'\\tAccuracy: {accuracy_score(y_true, y_pred)*100:8.2f}%')\n","  if (show_confusion_matrix):\n","    print('\\tConfusion matrix:\\n', confusion_matrix(y_true, y_pred))"],"metadata":{"id":"CFhzIXvlr0tz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets try some simple basic model on the data. DecisionTree and RandomForrest. As you will see it is a difficult task for them to get nice results. The result may differe from run to run due to incorporating a random process in prunning for DecisionTree and bagging in RandomForrest."],"metadata":{"id":"vSZ6jfWj-NLv"}},{"cell_type":"code","source":["base_models = [DecisionTreeClassifier(), RandomForestClassifier()]\n","\n","for model in base_models:\n","  model.fit(train_x, train_y)\n","  y_pred = model.predict(test_x)\n","\n","  print(type(model).__name__)\n","  compute_metrics(test_y, y_pred)"],"metadata":{"id":"WBvXDrvFtdpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Neural Network models\n","Lets try some basic neural network model for this task. The first is a classical dense network with two hidden layers and dropout optimization, that is able to best the Randomforrest classifier."],"metadata":{"id":"LghoxL4M-nd3"}},{"cell_type":"code","source":["def show_history(history):\n","    plt.figure()\n","    for key in history.history.keys():\n","        plt.plot(history.epoch, history.history[key], label=key)\n","    plt.legend()\n","    plt.tight_layout()"],"metadata":{"id":"4CZWihLEv2WV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Dense(256, activation='relu', input_shape=train_x[0].shape),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Dense(256, activation='relu', input_shape=train_x[0].shape),\n","    keras.layers.Dropout(0.2),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(optimizer='adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = ['accuracy'])"],"metadata":{"id":"BRPkXr-luUZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=10, batch_size=32)\n","show_history(history)"],"metadata":{"id":"xw2iMh2iwB3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To test Convolution in single dimension we need to reshape the data to have the proper format. The format is the same to recurrent data (accidentaly) and must be in a format $(vectors,length,planes)$."],"metadata":{"id":"_i-iOz29_Gls"}},{"cell_type":"code","source":["train_xc = np.reshape(train_x, (*train_x.shape, 1))\n","test_xc = np.reshape(test_x, (*test_x.shape, 1))\n","train_xc.shape, test_xc.shape"],"metadata":{"id":"eENnNrX80Yf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets try the single convolution layer as a input mapping that generates a huge number of weights for Dense layers after flattening. The results are not excelent. "],"metadata":{"id":"cCbC458r_lkc"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=train_xc[0].shape),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(optimizer='adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = ['accuracy'])"],"metadata":{"id":"qrqnu7dMz96L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_xc, train_y, validation_data=(test_xc, test_y), epochs=10, batch_size=32)\n","show_history(history)"],"metadata":{"id":"IfJgg3ol1thW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The slightly more complicated model is able to beat all previous models with smallel number of weight needed."],"metadata":{"id":"IGlwyEg9AHE1"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=train_xc[0].shape),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n","    keras.layers.MaxPool1D(2),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=train_xc[0].shape),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(optimizer='adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = ['accuracy'])"],"metadata":{"id":"5GT-keFLAD51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_xc, train_y, validation_data=(test_xc, test_y), epochs=10, batch_size=32)\n","show_history(history)"],"metadata":{"id":"zf0ZsGW-AFtP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Even more capable model with more pooling layers with 1/4 weight of the previsou model i able to achieve more than 90% of the accuracy. It has a one big drawback that reduce its ability to achieve better results. "],"metadata":{"id":"GLSynV9PAU7j"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=train_xc[0].shape),\n","    keras.layers.MaxPool1D(2),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n","    keras.layers.MaxPool1D(2),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=train_xc[0].shape),\n","    keras.layers.MaxPool1D(2),\n","    keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(optimizer='adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = ['accuracy'])"],"metadata":{"id":"uC4a_qTFpqeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_xc, train_y, validation_data=(test_xc, test_y), epochs=10, batch_size=32)\n","show_history(history)"],"metadata":{"id":"PDDvCfTap6Yi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Recurrent models\n","Lets focus on a more time series look on the data and use a recurrent models on the data, that should be able to achieve a better results when used properly. "],"metadata":{"id":"TfjHwdrpBUQk"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.GRU(64, activation='tanh', input_shape=train_xc[0].shape, return_sequences=True),\n","    keras.layers.GRU(32, activation='tanh', input_shape=train_xc[0].shape, return_sequences=True),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(2, activation='softmax')\n","])\n","\n","model.summary()\n","model.compile(optimizer='adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics = ['accuracy'])"],"metadata":{"id":"RQW5Of0SrgZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_xc, train_y, validation_data=(test_xc, test_y), epochs=10, batch_size=32)\n","show_history(history)"],"metadata":{"id":"i8PFvHkmr1nK"},"execution_count":null,"outputs":[]}]}
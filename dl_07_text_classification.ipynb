{"cells":[{"cell_type":"markdown","metadata":{"id":"ilthBvnZCQto"},"source":["# Deep Learning - Exercise 7\n","This lecture is focused on the more advanced examples of the RNN usage for text data anylysis.\n","\n","We will deal with the sentiment analysis task using Twitter data.\n","\n","You can download the dataset from [this link](https://github.com/MohamedAfham/Twitter-Sentiment-Analysis-Supervised-Learning/tree/master/Data)\n"]},{"cell_type":"markdown","metadata":{"id":"Fi2Jwhs35Itq"},"source":["[Open in Google colab](https://colab.research.google.com/github/rasvob/2020-21-ARD/blob/master/abd_07.ipynb)\n","[Download from Github](https://github.com/rasvob/2020-21-ARD/blob/master/abd_07.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BjuZNOOepoH"},"outputs":[],"source":["import matplotlib.pyplot as plt # plotting\n","import matplotlib.image as mpimg # images\n","import numpy as np #numpy\n","import seaborn as sns\n","import tensorflow as tf #use tensorflow \n","import tensorflow.keras as keras # required for high level applications\n","from sklearn.model_selection import train_test_split # split for validation sets\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n","from sklearn.preprocessing import normalize # normalization of the matrix\n","import scipy\n","import pandas as pd\n","\n","tf.version.VERSION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azKf1GfCepoI"},"outputs":[],"source":["import unicodedata, re, string\n","import nltk\n","from textblob import TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ud6XDYbepoI"},"outputs":[],"source":["def show_history(history):\n","    plt.figure()\n","    for key in history.history.keys():\n","        plt.plot(history.epoch, history.history[key], label=key)\n","    plt.legend()\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZMXmPdRepoJ"},"outputs":[],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpYvwbWzepoJ"},"outputs":[],"source":["df = pd.read_csv('https://github.com/jplatos/VSB-FEI-Deep-Learning/raw/main/datasets/train_tweets.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kp2JZMbKepoK"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"COE6V84VepoK"},"source":["# Let's take a look at the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqKpbylZepoK"},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{"id":"tKo7L_pIepoL"},"source":["## We can see that the classification task is highly imbalanced, because we have only 2242 negative tweets compared with positive one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YyfHfhLnepoL"},"outputs":[],"source":["sns.countplot(x='label', data=df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ykYF1uJepoL"},"outputs":[],"source":["df.label.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-4-W3wVepoL"},"outputs":[],"source":["df['length'] = df.tweet.apply(len)"]},{"cell_type":"markdown","metadata":{"id":"fS-4LPQCepoL"},"source":["### We can see that the sentences are of similar lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7SfOQ-YepoL"},"outputs":[],"source":["sns.barplot(x='label', y='length', data = df)"]},{"cell_type":"markdown","metadata":{"id":"mEqQcPicepoM"},"source":["# We can see that the text data are full of noise\n","\n","- Social posts suffer the most from this effect\n","- The text is full of hashtags, emojis, @mentions and so on\n","- These parts usually don't influence the sentiment score by much\n","- Although most advanced models usually extract even this features because e.g. emojis can help you with the sarcasm understanding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4WK9ChgepoM"},"outputs":[],"source":["for x in df.loc[:10, 'tweet']:\n","    print(x)\n","    print('---------')"]},{"cell_type":"markdown","metadata":{"id":"6cNphVBXepoM"},"source":["## Stemming\n","Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”,\n","\n","## Lemmatization \n","Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n","\n","Examples of lemmatization:\n","\n","- rocks : rock\n","- corpora : corpus\n","- better : good\n","\n","## Both techiques can be used in the preprocessing pipeline\n","You have to decide if it is beneficial to you, because this steps leads to some generalization of the data by itself. You will definitely lose some pieces of the information. If you use some form of embedding like Word2Vec or Glove, it is better to skip this steps because the embedding vocabulary skipped it as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y69oi_FTepoM"},"outputs":[],"source":["def remove_non_ascii(words):\n","    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","        new_words.append(new_word)\n","    return new_words\n","\n","def to_lowercase(words):\n","    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = word.lower()\n","        new_words.append(new_word)\n","    return new_words\n","\n","def remove_punctuation(words):\n","    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = re.sub(r'[^\\w\\s]', '', word)\n","        if new_word != '':\n","            new_words.append(new_word)\n","    return new_words\n","\n","def remove_numbers(words):\n","    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = re.sub(\"\\d+\", \"\", word)\n","        if new_word != '':\n","            new_words.append(new_word)\n","    return new_words\n","\n","def remove_stopwords(words):\n","    \"\"\"Remove stop words from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        if word not in stopwords.words('english'):\n","            new_words.append(word)\n","    return new_words\n","\n","def stem_words(words):\n","    \"\"\"Stem words in list of tokenized words\"\"\"\n","    stemmer = LancasterStemmer()\n","    stems = []\n","    for word in words:\n","        stem = stemmer.stem(word)\n","        stems.append(stem)\n","    return stems\n","\n","def lemmatize_verbs(words):\n","    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = []\n","    for word in words:\n","        lemma = lemmatizer.lemmatize(word, pos='v')\n","        lemmas.append(lemma)\n","    return lemmas\n","\n","def normalize(words):\n","    words = remove_non_ascii(words)\n","    words = to_lowercase(words)\n","# words = remove_punctuation(words)\n","    words = remove_numbers(words)\n","#    words = remove_stopwords(words)\n","    return words\n","\n","def form_sentence(tweet):\n","    tweet_blob = TextBlob(tweet)\n","    return tweet_blob.words"]},{"cell_type":"markdown","metadata":{"id":"mbEugrFtepoN"},"source":["# Tokenize sentences and remove puncuation by TextBlob library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_J0vcRJepoN"},"outputs":[],"source":["df['Words'] = df['tweet'].apply(form_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ce1G6Z2HepoN"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"5A0qoiUkepoN"},"source":["# Normalize sentences \n","- We want only ascii, lowercase and no numbers\n","\n","## You can experiments with different preprocess steps!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf5zZky-epoN"},"outputs":[],"source":["df['Words_normalized'] = df['Words'].apply(normalize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2tA_ENkepoN"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"n_vu092vepoO"},"source":["## Remove the 'user' word from tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaO9oG28epoO"},"outputs":[],"source":["df['Words_normalized_no_user'] = df['Words_normalized'].apply(lambda x: [y for y in x if 'user' not in y])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_YkacTdepoO"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"28MOqQuKepoO"},"source":["## We can see that no pre-processing is ideal and we have to fix some issues by ourselves\n","- e.g. n't splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDO70wzjepoO"},"outputs":[],"source":["print(df.tweet.iloc[1])\n","print(df.Words_normalized_no_user.iloc[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdXMZsQbepoO"},"outputs":[],"source":["def fix_nt(words):\n","    st_res = []\n","    for i in range(0, len(words) - 1):\n","        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n","            st_res.append(words[i]+(\"n't\"))\n","        else:\n","            if words[i] != \"n't\" and words[i] != \"nt\":\n","                st_res.append(words[i])\n","    return st_res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6V7phyoXepoO"},"outputs":[],"source":["df['Words_normalized_no_user_fixed'] = df['Words_normalized_no_user'].apply(fix_nt)"]},{"cell_type":"markdown","metadata":{"id":"dpacu-nHepoO"},"source":["## The issue is now fixed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQBUVSa4epoO"},"outputs":[],"source":["print(df.tweet.iloc[1])\n","print(df.Words_normalized_no_user.iloc[1])\n","print(df.Words_normalized_no_user_fixed.iloc[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvSktN6XepoO"},"outputs":[],"source":["df['Clean_text'] = df['Words_normalized_no_user_fixed'].apply(lambda x: \" \".join(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mc5Sj3TYepoO"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"hE3BICbjepoP"},"source":["# Let's take a look at the most common words in corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8-5MfnYepoP"},"outputs":[],"source":["import itertools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tMjUfOVepoP"},"outputs":[],"source":["all_words = list(itertools.chain(*df.Words_normalized_no_user_fixed))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnjIzgCcepoP"},"outputs":[],"source":["dist = nltk.FreqDist(all_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAenTZLKepoP"},"outputs":[],"source":["dist"]},{"cell_type":"markdown","metadata":{"id":"lFxSAnn6epoP"},"source":["### How many words do we have?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOji32MQepoP"},"outputs":[],"source":["len(dist)"]},{"cell_type":"markdown","metadata":{"id":"qHZrpI1XepoP"},"source":["### How long is the longest tweet?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWy4iOMRepoP"},"outputs":[],"source":["max(df.Words_normalized_no_user_fixed.apply(len))"]},{"cell_type":"markdown","metadata":{"id":"UX5kyVTuepoP"},"source":["# We will use the TextVectorization layer for creating vector model from our text data\n","For those of you who are interested in the topic there is very good [article on Medium](https://towardsdatascience.com/you-should-try-the-new-tensorflows-textvectorization-layer-a80b3c6b00ee) about the layer and its parameters.\n","\n","There is of course a [documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) about the layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7B_kglNjepoP"},"outputs":[],"source":["from tensorflow import string as tf_string\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"To3OFVNQepoP"},"outputs":[],"source":["embedding_dim = 128 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n","vocab_size = 10000 # Number of unique tokens in vocabulary\n","sequence_length = 30 # Output dimension after vectorizing - words in vectorited representation are independent\n","\n","vect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n","vect_layer.adapt(df.Clean_text.values)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5faf81BBepoP"},"source":["### We will split our dataset to train and test parts with stratification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15LygMWQepoP"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(df.Clean_text, df.label, test_size=0.20, random_state=13, stratify=df.label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWwojaaMepoP"},"outputs":[],"source":["X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=13, stratify=y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6YfCjQoepoP"},"outputs":[],"source":["print(X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZDwK5w2epoQ"},"outputs":[],"source":["print('Train')\n","print(y_train.value_counts())\n","print('Test')\n","print(y_test.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdTCDT8YepoQ"},"outputs":[],"source":["print('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\n","print('Vocabulary shape: ', len(vect_layer.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXm6_G42epoQ"},"outputs":[],"source":["input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n","x_v = vect_layer(input_layer)\n","emb = keras.layers.Embedding(vocab_size, embedding_dim)(x_v)\n","x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(emb)\n","x = keras.layers.GRU(64, activation='relu', return_sequences=True)(x)\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(64, 'relu')(x)\n","x = keras.layers.Dense(32, 'relu')(x)\n","x = keras.layers.Dropout(0.2)(x)\n","output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n","\n","model = keras.Model(input_layer, output_layer)\n","model.summary()\n","\n","model.compile(optimizer='rmsprop', loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXR09ffYepoQ"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 128\n","epochs = 5\n","history = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ns8a5HBfepoQ"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FH_O1ct4epoQ"},"outputs":[],"source":["y_test_loss, accuracy = model.evaluate(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYBAsmk7epoQ"},"outputs":[],"source":["y_pred = model.predict(X_test).ravel()"]},{"cell_type":"markdown","metadata":{"id":"DqZGE-kcepoQ"},"source":["#### Sigmoid function gives us real number in range <0, 1>.\n","\n","#### We need to map this valus to classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh1xdur6epoQ"},"outputs":[],"source":["y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGrj9v-6epoQ"},"outputs":[],"source":["y_pred = [1 if x >= 0.5 else 0 for x in y_pred]"]},{"cell_type":"markdown","metadata":{"id":"RFnhRisUepoQ"},"source":["# We can see that accuracy is not the best metric in the imbalanced situation - why?\n","There are many more metrics we can use and one of the most common in this situation is the F1 Score, see [this](https://en.wikipedia.org/wiki/F-score) and [this](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) for more info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMQcsFdQepoQ"},"outputs":[],"source":["accuracy_score(y_true=y_test, y_pred=y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkQ8AOqnepoQ"},"outputs":[],"source":["f1_score(y_true=y_test, y_pred=y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQ0KOF0depoQ"},"outputs":[],"source":["print(classification_report(y_true=y_test, y_pred=y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PB9AeO7TepoR"},"outputs":[],"source":["print(confusion_matrix(y_true=y_test, y_pred=y_pred))"]},{"cell_type":"markdown","metadata":{"id":"AysFQHPcepoR"},"source":["# We don't have to train our own embedding\n","There are multiple embeddings available online which were trained on very large corpuses e.g. Wikipedia. Good examples are Word2Vec, Glove or FastText. These embeddings contains fixed length vectors for words in the vocabulary.\n","\n","We will use GloVe embedding with 50 dimensional embedding vectors. For more details see [this](https://nlp.stanford.edu/projects/glove/).\n","You can download zip with vectors from [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip) ~ 800 MB\n","\n","#### Beware that the original text corpus was more general than the specific social media text data, so if you deal with very specific domains it may be beneficial to train your own embedding or at least fine tune existing one."]},{"cell_type":"markdown","metadata":{"id":"yUrnxRoeepoR"},"source":["# We need to download the embedding files\n","\n","~~~\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip\n","~~~\n","\n","The file glove.6B.50d.txt is also available in [Github](https://github.com/jplatos/VSB-FEI-Deep-Learning/raw/main/datasets/glove.6B.50d.zip) repository."]},{"cell_type":"code","source":["!wget https://github.com/jplatos/VSB-FEI-Deep-Learning/raw/main/datasets/glove.6B.50d.zip\n","!unzip -q glove.6B.50d.zip"],"metadata":{"id":"E0Ex1knTnDmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qKjGEgYepoR"},"source":["# First we need to load the file to memory and create embedding dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkLvR4BMepoR"},"outputs":[],"source":["path_to_glove_file = 'glove.6B.50d.txt'\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA-XxyEwepoR"},"outputs":[],"source":["embeddings_index['analysis']"]},{"cell_type":"markdown","metadata":{"id":"xQzI38zXepoR"},"source":["## We need to get the voacabulary from the Vectorizer and the integer indexes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxXp4Nm5epoR"},"outputs":[],"source":["embedding_dim = 50 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\n","vocab_size = 10000 # Number of unique tokens in vocabulary\n","sequence_length = 20 # Output dimension after vectorizing - words in vectorited representation are independent\n","\n","vect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n","vect_layer.adapt(df.Clean_text.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX4Ao-KvepoR"},"outputs":[],"source":["voc = vect_layer.get_vocabulary()\n","word_index = dict(zip(voc, range(len(voc))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FspV4K4kepoR"},"outputs":[],"source":["voc[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSyj490iepoR"},"outputs":[],"source":["word_index['the']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrc_Fp9pepoR"},"outputs":[],"source":["embeddings_index['the']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heD7xNFxepoR"},"outputs":[],"source":["num_tokens = len(voc) + 2\n","hits = 0\n","misses = 0\n","\n","# Prepare embedding matrix\n","embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in embedding index will be all-zeros.\n","        # This includes the representation for \"padding\" and \"OOV\"\n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        misses += 1\n","print(\"Converted %d words (%d misses)\" % (hits, misses))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZM6DbNjepoS"},"outputs":[],"source":["embedding_matrix[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJ4z9NFaepoS"},"outputs":[],"source":["input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\n","x_v = vect_layer(input_layer)\n","emb = keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)(x_v)\n","x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(emb)\n","x = keras.layers.GRU(64, activation='relu', return_sequences=False)(x)\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(64, 'relu')(x)\n","x = keras.layers.Dense(32, 'relu')(x)\n","x = keras.layers.Dropout(0.2)(x)\n","output_layer = keras.layers.Dense(1, 'sigmoid')(x)\n","\n","model = keras.Model(input_layer, output_layer)\n","model.summary()\n","\n","model.compile(optimizer='rmsprop', loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGNyNpnsepoS"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 128\n","epochs = 5\n","history = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKqJ847PepoS"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSXPuIwqepoS"},"outputs":[],"source":["y_pred = model.predict(X_test).ravel()\n","y_pred = [1 if x >= 0.5 else 0 for x in y_pred]\n","print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred)}')\n","print(f'F1 Score: {f1_score(y_true=y_test, y_pred=y_pred)}')\n","print(confusion_matrix(y_true=y_test, y_pred=y_pred))"]},{"cell_type":"markdown","metadata":{"id":"L9VyX_EyepoS"},"source":["# Task for the lecture\n"," - Try to create your own architecture\n"," - Experiment a little - try different batch sizes, optimimizers, etc\n"," - Send me a link to the Colab notebook with results and description of what you did and your final solution!\n"," \n","# There is a competition for bonus points this week!\n","- Everyone who will send me a correct solution will be included in the F1 - Score toplist\n","- Deadline for the competition submission is Monday 14th at 23:59\n","- The toplist will be publicly available on Wednesday\n","- There is no limitation in used layers (LSTM, CNN, ...), optimizers, etc. \n","- You can use any model architecture from the internet including transfer learning,\n","- The only limitation is that the model has to be trained/fine-tuned on Colab/Kaggle/Your machine so online sentiment scoring services are forbidden!\n","\n","## The winner with the best F1 - Score on test set will be awarded with 5 bonus points\n","- The test set is the same as we used in the lecture"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://github.com/rasvob/2020-21-ARD/blob/master/abd_07.ipynb","timestamp":1668367968083}]},"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":0}
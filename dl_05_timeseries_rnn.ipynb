{"cells":[{"cell_type":"markdown","metadata":{"id":"ilthBvnZCQto"},"source":["# Deep Learning - Exercise 5/6 - Time Series Forecasting using RNN\n","This lecture is focused on the basic examples of the RNN usage for time series forecasting.\n","\n","We will use Amazon stocks dataset from Yahoo finance. You can take look at this [this](https://finance.yahoo.com/quote/AMZN?p=AMZN)\n","\n","Other datasets are also available, we will show you how to create your own as well.\n"]},{"cell_type":"markdown","metadata":{"id":"Fi2Jwhs35Itq"},"source":["[Open in Google colab](https://colab.research.google.com/github/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_05_timeseries_rnn.ipynb)\n","[Download from Github](https://raw.githubusercontent.com/jplatos/VSB-FEI-Deep-Learning/main/dl_05_timeseries_rnn.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8mm69x7aeBx"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import matplotlib.pyplot as plt # plotting\n","import matplotlib.image as mpimg # images\n","import numpy as np #numpy\n","import seaborn as sns\n","import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n","import tensorflow.keras as keras # required for high level applications\n","from sklearn.model_selection import train_test_split # split for validation sets\n","from sklearn.preprocessing import normalize, MinMaxScaler, StandardScaler # normalization of the matrix\n","import scipy\n","import pandas as pd\n","\n","tf.version.VERSION"]},{"cell_type":"markdown","source":["A utility function that show Learning progress."],"metadata":{"id":"GM0l97PndSfS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrokD_0kaeBy"},"outputs":[],"source":["def show_history(history):\n","    plt.figure()\n","    for key in history.history.keys():\n","        plt.plot(history.epoch, history.history[key], label=key)\n","    plt.legend()\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"1OabJ7SJaeBz"},"source":["# We have prepared three datasets for your experiments\n","## AAPL = Apple\n","## AMZN = Amazon\n","## SNE = Sony\n","\n","## You are not limited to them, you can create your own datasets as well\n","## The prepared data covers period of the last 10 years with daily sampling frequency ~ 2500 values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVglr56uaeBz"},"outputs":[],"source":["df = pd.read_csv('https://raw.githubusercontent.com/rasvob/2020-21-ARD/master/datasets/SNE.csv')\n","df.index = pd.to_datetime(df.Date)\n","df = df.drop('Date', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JojNlfO_aeBz"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"zSi16Pi6aeB0"},"source":["# We are interested only in the Open column, which we will forecast\n","## We will deal with univariate time series forecasting in this lecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wO-qxW1waeB0"},"outputs":[],"source":["df = pd.DataFrame({'Price': df.iloc[:, 0]})"]},{"cell_type":"markdown","metadata":{"id":"oKez_yq9aeB0"},"source":["# The first step in every analysis task is taking a look at the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCrm_tMAaeB0"},"outputs":[],"source":["def show_timeseries(df):\n","    figsize = 20\n","    plt.figure(figsize=(figsize,figsize/2))\n","    plt.plot(df.index, df)\n","    plt.ylabel('Price ($)')\n","    plt.xlabel('Datetime')"]},{"cell_type":"markdown","metadata":{"id":"SSdWhT65aeB0"},"source":["# We can see that the stock price has significant trend and there are short term fluctuations as well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mJFm0RRaeB1"},"outputs":[],"source":["show_timeseries(df)"]},{"cell_type":"markdown","metadata":{"id":"cLaP6OIoaeB1"},"source":["# Let's take a look at data for the last month\n","## The changes in the short time periods are not as significant as in the lont term scenario"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qAbY80OaeB1"},"outputs":[],"source":["viz_subset = df.iloc[-30:]\n","show_timeseries(viz_subset)"]},{"cell_type":"markdown","metadata":{"id":"2Gjt1ishaeB1"},"source":["# We are usually interested in the dependency of the current value on the past values\n","## Auto-correlation function can help us with it\n","## We can vizualize the function values as well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6Gb_EYqaeB1"},"outputs":[],"source":["figsize = 20\n","plt.figure(figsize=(figsize,figsize/2))\n","plt.acorr(df.Price, maxlags=60)"]},{"cell_type":"markdown","metadata":{"id":"vMgK5relaeB1"},"source":["# The time series has very high auto-correlation function values\n","## We can see that the ACF values are constantly lower for the longer lag values\n","## This means that we are mainly interested in the last few weeks/months of the data"]},{"cell_type":"markdown","metadata":{"id":"THg97RXcaeB1"},"source":["# We worked with the classification models in the last few weeks\n","We evaluated quality of every created models based on its accuracy.\n","\n","Accuracy is only one of the many metrics for the classification task but it is the simples one (take a look at the F1-Score, AuC/ROC, Precision/recall metrics if you are interested in this area).\n","\n","Forecasting and regression tasks are not different - we have to evaluate model quality as well, butut now we use different types of metrics - most basic ones are MAE, RMSE which we already encountered.\n","There are many more metrics - R2, MAPE, sMAPE etc.\n","\n","We have prepared the evaluation functions API for you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXWXSHNcaeB2"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","\"\"\"\n","Computes MAPE\n","\"\"\"\n","def mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","\"\"\"\n","Computes SMAPE\n","\"\"\"\n","def symetric_mean_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n","    return np.mean(np.abs((y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred))/2.0))) * 100\n","\n","\"\"\"\n","Computes MAE, MSE, MAPE, SMAPE, R2\n","\"\"\"\n","def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:\n","    y_true, y_pred = df['y_true'].values, df['y_pred'].values\n","    return compute_metrics_raw(y_true, y_pred)\n","\n","def compute_metrics_raw(y_true: pd.Series, y_pred: pd.Series) -> pd.DataFrame:\n","    mae, mse, mape, smape, r2 = mean_absolute_error(y_true=y_true, y_pred=y_pred), mean_squared_error(y_true=y_true, y_pred=y_pred), mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), symetric_mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred), r2_score(y_true=y_true, y_pred=y_pred)\n","    return pd.DataFrame.from_records([{'MAE': mae, 'MSE': mse, 'MAPE': mape, 'SMAPE': smape, 'R2': r2}], index=[0])"]},{"cell_type":"markdown","metadata":{"id":"KeI-vu-YaeB2"},"source":["# The time series data are, surprisingly, time dependant\n","Time dependency means that we can't for example use Cross-validation or train/valid/test split without minding the time aspect. So to say - we can't split data randomly. We need to use test data from the period after the training set. If we don't take the time into account we will end up with leaking the data from test set to train one. Basically we let the model taking a look into the future. Which is obviously not wanted feature of the model, because there is no magic oracle available in the real-life forecasting scenarios."]},{"cell_type":"markdown","metadata":{"id":"KVE3ab_PaeB2"},"source":["# We will use the whole period of 2010 to 2018 as the training dataset\n","## First half of the 2019 will be used for validation and the second half as the testing dataset"]},{"cell_type":"code","source":["scaler = StandardScaler()\n","scaler.fit(df[df.index < '2019-01-01'].Price.values.reshape(-1,1))"],"metadata":{"id":"ZX2l4PDhW5ow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Price'] = scaler.transform(df.Price.values.reshape(-1,1))[:,0]"],"metadata":{"id":"eqyH0VG3XqUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"-fNTqISeZ6EY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1yH7pygaeB2"},"source":["# Our task will be to forecast the next day stock price"]},{"cell_type":"markdown","metadata":{"id":"m0cvaV6_aeB3"},"source":["### Our first task is data preprocessing and feature creating - we will use past N values as the model input\n","#### We can tune this parameter - we will start with 2 months worth of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySMnPYP_aeB3"},"outputs":[],"source":["maxlag = 60\n","df_orig = df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWa7LCzUaeB3"},"outputs":[],"source":["for x in range(maxlag, 1, -1):\n","    df[f'Price_lag_{x}'] = df.Price.shift(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LV2GMNNaeB3"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"yMdRsJbkaeB3"},"source":["### We can see that now we have a lot of NaN values - we dont know values of the past prices before dataset start - we will have to drop this rows"]},{"cell_type":"markdown","metadata":{"id":"Tk6T1aygaeB3"},"source":["# Now we will create the three datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAPtcxS4aeB3"},"outputs":[],"source":["X, y = df[df.index < '2020-01-01'].iloc[:, 1:], df[df.index < '2020-01-01'].Price"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9qHiug2aeB3"},"outputs":[],"source":["X_train, y_train = X[X.index.year < 2019], y[X.index.year < 2019]\n","X_valid, y_valid = X[(X.index.year == 2019) & (X.index.month <= 6)], y[(X.index.year == 2019) & (X.index.month <= 6)]\n","X_test, y_test = X[(X.index.year == 2019) & (X.index.month > 6)], y[(X.index.year == 2019) & (X.index.month > 6)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Anj0NAhUaeB3"},"outputs":[],"source":["X_train = X_train.dropna()\n","y_train = y_train[X_train.index]"]},{"cell_type":"markdown","metadata":{"id":"3n8f7xrxaeB3"},"source":["## The data has to have the same shapes in the first index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6QVFVipaeB3"},"outputs":[],"source":["print(X_train.shape, y_train.shape)\n","print(X_valid.shape, y_valid.shape)\n","print(X_test.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"_zHew4rraeB4"},"source":["### We can verify the defined intervals for the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6XYgiUlaeB4"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFPHL_cBaeB4"},"outputs":[],"source":["print('TRAIN\\n', y_train, '\\nVALID\\n',y_valid, '\\nTEST\\n',y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqVF82_iaeB4"},"outputs":[],"source":["X_test_idx = X_test.index"]},{"cell_type":"markdown","metadata":{"id":"cJJDnFLtaeB4"},"source":["# Now we can create the model and evaluate it"]},{"cell_type":"markdown","metadata":{"id":"ppMQQv1taeB4"},"source":["### We will start with the simple fully-connected network as baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1XwZwOZaeB4"},"outputs":[],"source":["input_layer = keras.layers.Input(shape=X_train.shape[1])\n","x = keras.layers.Dense(64, activation='relu')(input_layer)\n","output_layer = keras.layers.Dense(1, activation='linear')(x)\n","\n","model = keras.Model(input_layer, output_layer)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k94pHSIWaeB4"},"outputs":[],"source":["model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLvuFf1zaeB4"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 32\n","epochs = 30\n","history = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adP4cSS7aeB4"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Te-xsODaeB5"},"outputs":[],"source":["y_pred = model.predict(X_test.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68pb6GenaeB5"},"outputs":[],"source":["df_res = pd.DataFrame({'y_pred': y_pred.reshape(-1), 'y_true': y_test})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnXeK-hWaeB5"},"outputs":[],"source":["df_res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sELpoPpJaeB5"},"outputs":[],"source":["compute_metrics(df_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQ9tkpgNaeB5"},"outputs":[],"source":["def show_forecasts(df_res):\n","    figsize = 20\n","    plt.figure(figsize=(figsize,figsize/2))\n","    plt.plot(df_res.index, df_res.y_true, color='red')\n","    plt.plot(df_res.index, df_res.y_pred, color='green')\n","    plt.ylabel('Price ($)')\n","    plt.xlabel('Datetime')"]},{"cell_type":"markdown","metadata":{"id":"xDXPRa65aeB5"},"source":["# We can see that the error is quite high but even the relatively simple model captured overlaying trend of the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz2QBFmzaeB5"},"outputs":[],"source":["show_forecasts(df_res)"]},{"cell_type":"markdown","metadata":{"id":"a79EVuVpaeB5"},"source":["# Now we will try more complex model - recurrent one"]},{"cell_type":"markdown","metadata":{"id":"Xcj5T6Z_aeB5"},"source":["The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n","\n","We can transform the prepared train and test input data into the expected structure using numpy.reshape()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkH14IIeaeB5"},"outputs":[],"source":["X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n","X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n","X_valid = X_valid.values.reshape(X_valid.shape[0], X_valid.shape[1], 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mN2rV4qkaeB5"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qclrGDlbaeB5"},"outputs":[],"source":["inp = keras.layers.Input(shape=X_train.shape[1:])\n","x = keras.layers.LSTM(64, activation='relu')(inp)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dense(64, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","output_layer = keras.layers.Dense(1, activation='linear')(x)\n","\n","model = keras.Model(inp, output_layer)\n","model.summary()\n","model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2OP--fuaeB5"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 32\n","epochs = 30\n","history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YHufsJoaeB5"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-N2ODrYXaeB6"},"outputs":[],"source":["y_pred = model.predict(X_test)\n","df_res = pd.DataFrame({'y_pred': y_pred.reshape(-1), 'y_true': y_test}).dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cIqfCDmaeB6"},"outputs":[],"source":["compute_metrics(df_res)"]},{"cell_type":"code","source":["show_forecasts(df_res)"],"metadata":{"id":"PTeNBB1lCgJQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Frr3qP2GaeB6"},"source":["# We can stack multiple LSTM layers on each other"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4wwUXNQaeB6"},"outputs":[],"source":["inp = keras.layers.Input(shape=X_train.shape[1:])\n","x = keras.layers.LSTM(256, activation='relu', return_sequences=True)(inp)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.LSTM(256, activation='relu', return_sequences=True)(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Flatten(input_shape=(59, 256))(x)\n","x = keras.layers.Dense(256, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dense(128, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dropout(0.2)(x)\n","output_layer = keras.layers.Dense(1, activation='linear')(x)\n","\n","model = keras.Model(inp, output_layer)\n","model.summary()\n","model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byawle5aaeB6"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 32\n","epochs = 50\n","history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8bb_Fv5aeB6"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbP9fPh4aeB6"},"outputs":[],"source":["y_pred = model.predict(X_test)\n","# y_pred_diff = y_pred.reshape(-1) + price_orig[X_test_idx].shift(1)\n","df_res = pd.DataFrame({'y_pred': y_pred.reshape(-1), 'y_true': y_test}).dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDx-TAXfaeB6"},"outputs":[],"source":["compute_metrics(df_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzXIfjlmaeB6"},"outputs":[],"source":["show_forecasts(df_res)"]},{"cell_type":"markdown","metadata":{"id":"n2twkuCoaeB6"},"source":["# And we can create really complex models based on LSTM - the situation with LSTM is the same as with every ANN in general - more complex model doesn't mean better solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGqkwAhoaeB6"},"outputs":[],"source":["inp = keras.layers.Input(shape=X_train.shape[1:])\n","x = keras.layers.LSTM(1024, activation='relu', return_sequences=True)(inp)\n","avg1 = keras.layers.GlobalAveragePooling1D()(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.LSTM(256, activation='relu', return_sequences=True)(x)\n","avg2 = keras.layers.GlobalAveragePooling1D()(x)\n","concat = keras.layers.Concatenate()([keras.layers.Flatten()(avg1), keras.layers.Flatten()(avg2), keras.layers.Flatten()(x)])\n","x = keras.layers.Dense(512, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dense(256, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dropout(0.2)(x)\n","x = keras.layers.Dense(64, activation='relu')(x)\n","output_layer = keras.layers.Dense(1, activation='linear')(x)\n","\n","model = keras.Model(inp, output_layer)\n","model.summary()\n","model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8avIjpDHaeB6"},"outputs":[],"source":["# for the experimenting purpose only the EarlyStopping criterion is present\n","es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 32\n","epochs = 50\n","history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cPvp9sYaeB6"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajNOB9xOaeB7"},"outputs":[],"source":["y_pred = model.predict(X_test)\n","df_res = pd.DataFrame({'y_pred': y_pred.reshape(-1), 'y_true': y_test}).dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgBClxuzaeB7"},"outputs":[],"source":["compute_metrics(df_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xj6N10YtaeB7"},"outputs":[],"source":["show_forecasts(df_res)"]},{"cell_type":"markdown","metadata":{"id":"UpJ9iO0yaeB7"},"source":["# We can see that the trend extrapolation can be an issue\n","## What if we could get rid of the trend?\n","The solution lies in the time series differencing - take a look at [this](https://otexts.com/fpp2/stationarity.html)\n","\n","## We won't work with the stock price directly but we will use differences between two consecutive values\n","### Beware the forecast horizon length!\n","The differencing term can't be shorter than the horizon length, you would create dependency between two forecasts. We have forecast horizon of length 1 - only one day."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcqlB2i2aeB7"},"outputs":[],"source":["maxlag = 60\n","df_diff = df_orig.copy()\n","df_diff.Price = df_diff.Price.diff(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ye8R9mFyaeB7"},"outputs":[],"source":["for x in range(1, maxlag):\n","    df_diff[f'Price_lag_{x}'] = df_diff.Price.shift(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKtN8cAdaeB7"},"outputs":[],"source":["df_diff.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdMxnQO3aeB7"},"outputs":[],"source":["show_timeseries(df_diff.Price)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fnBtOgGaeB7"},"outputs":[],"source":["X, y = df_diff[df.index < '2020-01-01'].iloc[:, 1:], df_diff[df.index < '2020-01-01'].Price"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3X18_DiaeB7"},"outputs":[],"source":["X_train, y_train = X[X.index.year < 2019], y[X.index.year < 2019]\n","X_valid, y_valid = X[(X.index.year == 2019) & (X.index.month <= 6)], y[(X.index.year == 2019) & (X.index.month <= 6)]\n","X_test, y_test = X[(X.index.year == 2019) & (X.index.month > 6)], y[(X.index.year == 2019) & (X.index.month > 6)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUDW_iOzaeB7"},"outputs":[],"source":["X_train = X_train.dropna()\n","y_train = y_train[X_train.index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLBjHpOhaeB7"},"outputs":[],"source":["print('TRAIN\\n', y_train, '\\nVALID\\n',y_valid, '\\nTEST\\n',y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXzthn3naeB7"},"outputs":[],"source":["X_test_idx = X_test.index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2ie-C88aeB7"},"outputs":[],"source":["X_train = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n","X_test = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n","X_valid = X_valid.values.reshape(X_valid.shape[0], 1, X_valid.shape[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV7LRDE2aeB7"},"outputs":[],"source":["inp = keras.layers.Input(shape=X_train.shape[1:])\n","x = keras.layers.LSTM(1024, activation='relu', return_sequences=True)(inp)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.LSTM(512, activation='relu', return_sequences=True)(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dense(256, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dense(128, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.Dropout(0.2)(x)\n","output_layer = keras.layers.Dense(1, activation='linear')(x)\n","\n","model = keras.Model(inp, output_layer)\n","model.summary()\n","model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=['mae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hx03u9gzaeB8"},"outputs":[],"source":["es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n","\n","batch_size = 32\n","epochs = 50\n","history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), callbacks=[es], epochs=epochs, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8af-JtXaeB8"},"outputs":[],"source":["show_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1w2pLuq6aeB8"},"outputs":[],"source":["y_pred = model.predict(X_test)\n","y_pred_diff = y_pred.reshape(-1) + df_orig.Price[X_test_idx].shift(1)\n","df_res = pd.DataFrame({'y_pred': y_pred_diff, 'y_true': df_orig.Price[X_test_idx]}).dropna()"]},{"cell_type":"markdown","metadata":{"id":"6sVtVDjGaeB8"},"source":["# We can see that the LSTM alone is not a silver bullet - good preprocessing still matters even in the ANN area"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhulhgEMaeB8"},"outputs":[],"source":["compute_metrics(df_res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeB11QOmaeB8"},"outputs":[],"source":["show_forecasts(df_res)"]},{"cell_type":"markdown","metadata":{"id":"HVlvGBDtaeB8"},"source":["# Task for the lecture\n","\n"," - Choose other stock prices dataset\n"," - Try to create your own architecture using reccurent neural networks\n"," - Experiment a little - try different batch sizes, optimimizers, time lags as features, etc\n"," - Send me the Colab notebook with results and description what you did and your final solution!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://github.com/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_05_timeseries_rnn.ipynb","timestamp":1667068534660}]},"file_extension":".py","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":0}
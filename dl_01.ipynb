{"cells":[{"cell_type":"markdown","metadata":{"id":"8z4lsdTXjdMG"},"source":["# Deep Learning - Exercise 1\n","This lecture is about basics of the Tensorflow, we will discuss the minimal example on the MNIST dataset.\n","We also investigate a meaning of the validation sets and different complexity of the model. Moreover, we will look on the regulariozation and we will try to find optimal model for the MNIST dataset that is based on fully connected layers."]},{"cell_type":"markdown","metadata":{"id":"SCXkXCkBjdMH"},"source":["[Open in Google colab](https://colab.research.google.com/github/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_01.ipynb)\n","[Download from Github](https://github.com/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_01.ipynb)\n","\n","##### Remember to set **GPU** runtime in Colab!"]},{"cell_type":"markdown","metadata":{"id":"vGYmu2RCjdMH"},"source":["### Import of the TensorFlow\n","The main version of the TensorFlow (TF) is a in the Version package in the field VERSION\n","Since the TensformFlow 2.0 everything was encapsulaed under the KERAS api."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I798rQYbjdMI"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","tf.version.VERSION"]},{"cell_type":"markdown","metadata":{"id":"LWDIlb9UjdMI"},"source":["### Import a dataset\n","Datasets are stored in the keras.datasets submodule. Few testing datasets are stored here and installed together with the TF package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK0bAkspjdMI"},"outputs":[],"source":["# mnist is the basic dataset with handwritten digits\n","mnist = tf.keras.datasets.mnist\n","\n","# data from any dataset are loaded using the load_Data function\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# the data are in the form of 28x28 pixes with values 0-255.\n","print('Train data shape: ', x_train.shape)\n","print('Test data shape:  ', x_test.shape)"]},{"cell_type":"markdown","source":["The dataset consists of 60,000 training images and 10,000 testing images. All of the are 28x28 pixels. "],"metadata":{"id":"7jRqCq9jklBE"}},{"cell_type":"markdown","metadata":{"id":"ACxVYmJGjdMI"},"source":["### Lets look on the data how do they look like.\n","Look closely on the value scale - it os from 0 to 255 as usual in grayscale  images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZaKJJf3jdMJ"},"outputs":[],"source":["plt.figure()\n","plt.imshow(x_train[1])\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7Z_rfPvrjdMJ"},"source":["### The conversion into range 0-1 is done by the division\n","Lets normalize the values into the range \\(0,1\\) by dividing it 255."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCNrX5rqjdMJ"},"outputs":[],"source":["# The conversion into range 0-1 is done by the division\n","x_train, x_test = x_train / 255.0, x_test / 255.0"]},{"cell_type":"markdown","metadata":{"id":"tyhe6lUKjdMJ"},"source":["### Make better visualization of the data to better understand how complex they are"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIf8LNL7jdMJ"},"outputs":[],"source":["class_names = [str(x) for x in range(10)]\n","\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(x_train[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[y_train[i]])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"H3uY8cfbjdMJ"},"source":["### Basic model - a NN with very simple hierarchy\n","Model is created using layers, many layers exists in the [layer submodule](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n","\n","Each layer uses a activation functions collected in the [module nn](https://www.tensorflow.org/api_docs/python/tf/nn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FRfWkocjdMK"},"outputs":[],"source":["model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n","    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n","    keras.layers.Dense(10, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n","])\n","\n","model.summary() # prints the summary of the model"]},{"cell_type":"markdown","metadata":{"id":"XxkeKMLdjdMK"},"source":["### Compilation of the model\n","Each model need to be compiled to be able to fit to the data and predict the labels\n","\n","#### Optimizers\n","* Gradient descent\n","   * Works for the whole dataset and it is not suitable for large data\n","* Stochastic Gradiend Descent (SGD)\n","   * Approximate the real gradiend from selested subset of data (Stochasticity)\n","* Root Mean Square Propagation (RMSPRop)\n","   * Adapts the learnign rate with the running average of the recent gradients.\n","* Adamptive Moment Estimation (ADAM)\n","   * Averages gradients and secodn moment of the gradient and adapts the learning rate.\n","\n","#### Loss functions\n","* Mean Squared Error\n","   * a classical measure to be used in regression\n","   * a logarithmic version exists\n","* Mean Absolute Error (MAE)\n","   * take an absolute values instead of their squared version\n","* Binary classification Loss\n","   * a loss for binary problems only\n","   * predicts the probability of the class 1\n","* Binary Cross-Entropy\n","   * predict the class from the set {0,1}\n","   * requires a sigmoind activation function\n","* Categorical Cross-Entropy\n","   * default for mutli-class classification problems\n","   * requires the softmax function on output layer to compute probability of each layer\n","   * train labels have to be one-hot-encoded\n","* Sparse Categorical Cross-Entropy\n","   * the same as above but the tran lables are just labels not encoded.\n","\n","#### Metrics\n","* Regression metrics\n","   * Mean Squared Error (MSE)\n","   * Mean Absolute Error (MAE)\n","   * Mean Absolute Percentage Error (MAPE)\n","* Classification metrics\n","   * Binary Accuracy\n","   * Categorical Accuracy\n","   * Sparse Categorical Accuracy\n","   * Top k Categorical Accuracy\n","   * Sparse Top k Categorical Accuracy\n","   * Accuracy - a general version that is modified based on the data analyzed autmatically"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rh03ZvLjdMK"},"outputs":[],"source":["model.compile(optimizer='adam',\n","    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","    metrics = ['accuracy'],\n","    )"]},{"cell_type":"markdown","metadata":{"id":"lA09suMrjdMK"},"source":["### Model visualization\n","Model then compiles and it is ready for fitting to the data. \n","The model may be printed into image like the following image of our model:\n","\n","![model](https://github.com/jplatos/2019-2020-DA4/raw/master/images/da4_01_base.png \"Base model of the neural network\")"]},{"cell_type":"markdown","metadata":{"id":"WFIdCjwHjdMK"},"source":["### Fit the model to the input data\n","The *fit()* method fit the model to the data, the parameters are *data* and *labels* from the train set and number of *epoch* to be trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAKg-e-OjdML"},"outputs":[],"source":["# keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{"id":"An4hyfD7jdML"},"source":["## The .fit() API is pretty powerful\n","- It is common to use some sort of a callback, we will use ModelCheckpoint callback which saves the best weights configuration obtained during training so the overfitting at the final phase of training will be suppressed\n","- The best weights are determined using the validation loss value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUzeNn2JjdML"},"outputs":[],"source":["model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath='weights.best.hdf5',\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='auto',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixodDENSjdML"},"outputs":[],"source":["history = model.fit(x_train, y_train, validation_split=0.2, epochs=3, callbacks=[model_checkpoint_callback], batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj3duiFljdML"},"outputs":[],"source":["plt.figure()\n","for key in history.history.keys():\n","    plt.plot(history.epoch, history.history[key], label=key)\n","plt.legend()\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"O3hqCxJ8jdML"},"source":["## The weights needs to be loaded after the training is finished"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkuf66s5jdML"},"outputs":[],"source":["model.load_weights(\"weights.best.hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0QAkwi7jdMM"},"outputs":[],"source":["test_loss, test_acc = model.evaluate(x_test, y_test)\n","print('Test accuracy: ', test_acc)"]},{"cell_type":"markdown","metadata":{"id":"0N0cmLbvjdMM"},"source":["## Tasks for the rest of the lecture\n","\n","1. Investigate the quality of the model using validation sets.\n","2. Limit the overfitting using the regularization and dropout.\n","   1. L1 regularization (Lasso regression) - $\\lambda_1 \\sum_{i=0}^n \\left\\lvert w_i\\right\\rvert$\n","   2. L2 regularization (Ridge regression) - $\\lambda_2 \\sum_{i=0}^n \\left\\lVert w_i\\right\\rVert$\n","   3. Dropout\n","3. Prepare its own model that will classifiy the test data with precision more than 98 percent (or give it a try at least :))."]},{"cell_type":"markdown","metadata":{"id":"xwK61DWNjdMM"},"source":["# L1 and L2 regularization\n","- Similiar technique in the regression algorithms\n","- Usually loss-based approach (regularization norm is added to the loss function)\n","- Enforce the training process to steer towards relatively “simple” weights, which may make your model more generic\n","    - E.g. L1 - sparsity, correlation reduction - weights can be exactly 0\n","    - L2 - weights will be relatively small numbers - no extremly huge number for one weight compared to the other ones - no exact 0 weights\n","\n","## How does overfit look like? (Accuracy POV)\n","![overfit](https://github.com/rasvob/2020-21-ARD/raw/master/images/overfit_acc.png \"Overfit\")\n","\n","## Are there any other cases for regularization usage besides weights?\n","## Is L1 or L2 reg. related to Dropout somehow?\n","\n","## Keras API\n","- kernel_regularizer: weight reg.\n","- bias_regularizer: bias reg.\n","- activity_regularizer: layer's output reg.\n","\n","- E.g. tf.keras.layers.Dense(500, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n","\n","# Dropout\n","- Node sampling instead of edge sampling \n","- If a node is dropped, then all incoming and outgoing connections from that node need to be dropped as well\n","- We sample sub-networks from the original one - basically ensemble of networks\n","- There won't be some \"alpha\" node in the individual layer with huge weight coefficient\n","    - Responsibility for prediction will be shared among multiple nodes\n","\n","![dropout](https://github.com/rasvob/2020-21-ARD/raw/master/images/dropout.jpeg \"Dropout\")\n","\n","## Keras API\n","- keras.layers.Dropout(0.5)"]}],"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
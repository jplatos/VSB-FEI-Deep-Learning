{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdQiypC8ZTY9c6HYEVv8D2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Deep Learning Exercise 9 - Time Series Anomaly detection\n","\n","This exercise is about detection of the anomaly time series data. It does not focus on the time series elements, but on the whole time series that differs from the standard. \n","\n","Data we will use come from [Time Series Classification Website](https://www.timeseriesclassification.com/dataset.php), we will use sensor data from ECG datasets, but we will take a *Normal* class as a base and other classes as anomalies.\n","\n","\n","[Open in Google colab](https://colab.research.google.com/github/jplatos/VSB-FEI-Deep-Learning/blob/master/dl_09_time_series_anomalies.ipynb) [Download from Github](https://raw.githubusercontent.com/jplatos/VSB-FEI-Deep-Learning/main/dl_09_time_series_anomalies.ipynb)\n"],"metadata":{"id":"7cwivILA7JDk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbjJMA57NZLR"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.io.arff import loadarff \n","import plotly.express as px\n","pd.options.plotting.backend = \"plotly\"\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from sklearn.metrics import mean_absolute_error\n","\n","tf.version.VERSION"]},{"cell_type":"markdown","source":["### Dataset preparation\n","The dataset was taken from the above link and train and test data were joined together. Data contain 140 measurement of the ECG signal with 5 different classes. One *Normal* (class 1) and other classes are different hearth problems. \n","\n","Part of the class 1 will taken as a training and validation data. Other data will be taken as a testing data. The goal is to show, how to detect non-standard time series run that differs from the normal run."],"metadata":{"id":"38HuMEAUoz63"}},{"cell_type":"code","source":["df = pd.read_feather('https://github.com/jplatos/VSB-FEI-Deep-Learning/raw/main/datasets/ecg5000.feather')"],"metadata":{"id":"vVZqFtgdokQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"JCl5-4UgUPlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"7UMOESJqgHYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.sample(frac=1).reset_index(drop=True) # shuffle and reset data index"],"metadata":{"id":"skd_w6ghfyFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df # see the target value change"],"metadata":{"id":"gzfVUXifgErO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### See the frequency of each class in the data"],"metadata":{"id":"MEsvF2derAm3"}},{"cell_type":"code","source":["df.target.value_counts().plot.bar()"],"metadata":{"id":"OnE7TEAAdvGS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extract the normal class and the anomaly part of the data a drop the target values.\n","\n","Then reshape the data to the form that is suitable for the recurrent models *(number of measurement, number of features, record)*"],"metadata":{"id":"F84e0s2jq2xX"}},{"cell_type":"code","source":["normal = df[df.target==1].drop(columns=['target']).values\n","normal.shape"],"metadata":{"id":"T7KZ3kq1d1Y_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anomaly = df[df.target!=1].drop(columns=['target']).values\n","anomaly.shape"],"metadata":{"id":"pDvyDYGqQEMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sh = normal.shape\n","normal = np.reshape(normal, (sh[0],1, sh[1]))\n","normal.shape"],"metadata":{"id":"9o9BEngSnCAX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sh = anomaly.shape\n","anomaly = np.reshape(anomaly, (sh[0],1, sh[1]))\n","anomaly.shape"],"metadata":{"id":"X3Ctr8P2nZaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the *normal* data into three groups, *train*, *validation*, *test*. The ratio between the is in the form *70%:12%:18%*."],"metadata":{"id":"mUyZw3UdrsPP"}},{"cell_type":"code","source":["train, test = train_test_split(normal, test_size=0.3, random_state=42)\n","val, test = train_test_split(test, test_size=0.6, random_state=42)\n","train.shape, val.shape, test.shape"],"metadata":{"id":"B2FXP0V4le_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model preparation\n","\n","Model is very simple, it is a *Autoencoder* using the LSTM layers. The first layer encodes the input and the second encodes the data into a compressed form. The Decoder reconstruct the data into a original form.\n"],"metadata":{"id":"T11nVtkcsMOX"}},{"cell_type":"code","source":["seq_len = 140\n","features = 1\n","\n","model = keras.Sequential([\n","    # encoder\n","    keras.layers.LSTM(128, input_shape=train[0].shape, return_sequences = True),\n","    keras.layers.LSTM(32, input_shape=train[0].shape, return_sequences = True),\n","    # decoder\n","    keras.layers.LSTM(32, input_shape=train[0].shape, return_sequences = True),\n","    keras.layers.LSTM(128, input_shape=train[0].shape, return_sequences = True),\n","    keras.layers.Dense(seq_len, activation='linear')\n","])\n","model.summary()"],"metadata":{"id":"ez9YXE_Lpfos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(\n","    optimizer='adam',\n","    loss=tf.keras.losses.MeanSquaredError(),\n","    metrics='mae'\n",")\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n"],"metadata":{"id":"670idZ-SsMpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train, train, validation_data=(val, val), epochs=100, callbacks=[early_stopping])"],"metadata":{"id":"JWugPOLds7Q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["px.line(history.history)"],"metadata":{"id":"zUhUVdfcv0RO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training evaluation\n","Lets see the quality of reconstruction on the 42nd records in the training dataset, on the testing dataset and anomaly dataset. As may be seen, the reconstruction of the *Normal* dataset in train and test set is good, the quality of the anomaly show bigger differences. "],"metadata":{"id":"If4HLzhsY3HP"}},{"cell_type":"code","source":["index = 42\n","database = np.asarray([train[index], test[index], anomaly[index]])\n","predicted = model.predict(database)"],"metadata":{"id":"qSuYrtoLwPEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["px.line({'Real':database[0][0], 'Predicted':predicted[0][0]}, title='Train')"],"metadata":{"id":"HUlSN2XyxKbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["px.line({'Real':database[1][0], 'Predicted':predicted[1][0]}, title='Test')"],"metadata":{"id":"MhGHz_IxbNLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["px.line({'Real':database[2][0], 'Predicted':predicted[2][0]}, title='Anomaly')"],"metadata":{"id":"CzbrxKEpbWQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reconstruction error measure\n","The error in reconstruction is the mesure for anomaly detection. a MeanAbsoluteError is a good choice. Sometimes, a sum of absolute error is used, but the results is almost the same. \n","\n","First of all, lets see the histogram of differences on the train data."],"metadata":{"id":"NeGdyu8Mb4Ay"}},{"cell_type":"code","source":["train_pred = model.predict(train)\n","differences = [mean_absolute_error(real, pred) for (real, pred) in zip(train, train_pred)]\n","px.histogram(differences, title='Train data reconstruction error')"],"metadata":{"id":"yNbJ6HHk1YAC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then look at the reconstruction of on the test dataset."],"metadata":{"id":"MdK7UHPrcipE"}},{"cell_type":"code","source":["test_pred = model.predict(test)\n","differences = [mean_absolute_error(real, pred) for (real, pred) in zip(test, test_pred)]\n","px.histogram(differences, title='Test data reconstruction error')"],"metadata":{"id":"3IT6OLhQyFH3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Selection of the proper Threshold\n","The Threshold that distingushes between a normal time serie and a anomaly one is the critical part. Too high threshold leads to the high misclassification in false normal. Too low threshold leads to the high false anomaly rate. "],"metadata":{"id":"Smcj3srLcovd"}},{"cell_type":"code","source":["def evaluate_prediction(model, datasets, names, threshold):\n","  results = [f\"{'Dataset':>10}{'Normal':>10}{'Anomaly':>10}\\n\"]\n","  for (name, dataset) in zip(names, datasets):\n","    predicted = model.predict(dataset)\n","    differences = [mean_absolute_error(real, pred) for (real, pred) in zip(dataset, predicted)]\n","    results.append(f'{name:>10}{sum(l<=threshold for l in differences):>10}{sum(l>threshold for l in differences):>10}\\n')\n","  print(*results)\n"],"metadata":{"id":"nn0c8ZE93Jmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.2\n","evaluate_prediction(model, [train, test, anomaly], ['Train', 'Test', 'Anomaly'], threshold)"],"metadata":{"id":"8ocoQHO75w6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation\n","As may be seen, the threshold set to 0.2 leds to nice results where less than 1% of training samples are misclassified, 2% of test samples and 11% of anomaly samples are missclassified too. \n","\n","Everything dependson the setting of the LSTM autoencoder and the size of the compressed representation."],"metadata":{"id":"YmUkKtt0dVdh"}},{"cell_type":"markdown","source":["## Tasks\n","1. Try different size of the LSTM autoencoder.\n","2. Try a Conv1D eautoencoder to better cover the specificity of the encoder. \n","3. Select the proper of the Threshold value."],"metadata":{"id":"ghYoBEDWea3T"}},{"cell_type":"markdown","source":["### References\n","1. [LSTM Autoencoder for Anomaly Detection for ECG data\n"," by Abhishek Shah](https://medium.com/@jwbtmf/lstm-autoencoder-for-anomaly-detection-for-ecg-data-5c0b07d00e50)\n","2. [Real-Time Anomaly Detection With Python by Anthony Cavin](https://towardsdatascience.com/real-time-anomaly-detection-with-python-36e3455e84e2)"],"metadata":{"id":"E4LRITMoe8Ci"}}]}